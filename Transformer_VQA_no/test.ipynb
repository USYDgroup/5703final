{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinghao/anaconda3/envs/xinghao/lib/python3.10/site-packages/torchvision/transforms/transforms.py:891: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 1.9303,  1.9303,  1.9303,  ...,  1.8719,  1.4632,  1.5216],\n",
      "         [ 1.9303,  1.9303,  1.9303,  ...,  1.7990,  1.6092,  1.6968],\n",
      "         [ 1.8573,  1.9303,  1.9303,  ...,  1.8135,  1.8573,  1.9157],\n",
      "         ...,\n",
      "         [-0.4346, -0.3762, -0.1864,  ..., -0.6098, -0.7412, -0.6828],\n",
      "         [-0.4346, -0.3762, -0.2448,  ..., -0.7850, -0.6682, -0.3470],\n",
      "         [-0.6536, -0.5076, -0.1864,  ..., -0.6536, -0.4638, -0.2886]],\n",
      "\n",
      "        [[ 1.6247,  1.5646,  1.6697,  ...,  1.5046,  1.0844,  1.1444],\n",
      "         [ 1.5046,  1.5196,  1.5946,  ...,  1.4446,  1.2645,  1.3395],\n",
      "         [ 1.4295,  1.5046,  1.6096,  ...,  1.4295,  1.4896,  1.5196],\n",
      "         ...,\n",
      "         [-0.3714, -0.2663, -0.0712,  ..., -0.5815, -0.7016, -0.5815],\n",
      "         [-0.3564, -0.2663, -0.0412,  ..., -0.8216, -0.6715, -0.2213],\n",
      "         [-0.5365, -0.3864,  0.0789,  ..., -0.6565, -0.4164, -0.1012]],\n",
      "\n",
      "        [[ 1.0510,  1.0083,  1.0510,  ...,  1.0083,  0.6244,  0.6812],\n",
      "         [ 1.0652,  1.0794,  1.0083,  ...,  0.9088,  0.6955,  0.8092],\n",
      "         [ 0.9514,  1.0083,  1.0794,  ...,  0.9656,  1.0225,  1.1221],\n",
      "         ...,\n",
      "         [-0.2857, -0.1862,  0.0840,  ..., -0.2857, -0.4564, -0.4848],\n",
      "         [-0.1720, -0.1293,  0.0271,  ..., -0.5559, -0.4564, -0.2146],\n",
      "         [-0.3426, -0.2146,  0.0982,  ..., -0.4564, -0.2857, -0.1151]]]), 'what position is this man playing', ['pitcher[SEP]', 'catcher[SEP]'], [0.8999999999999999, 0.1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from dataset.vqa_dataset import vqa_dataset\n",
    "# from dataset.grounding_dataset import grounding_dataset\n",
    "\n",
    "from dataset.randaugment import RandomAugment\n",
    "from utils import MetricLogger\n",
    "from dataset import create_dataset, create_sampler, create_loader, vqa_collate_fn,vqa_dataset\n",
    "\n",
    "normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    \n",
    "train_transform = transforms.Compose([                        \n",
    "            transforms.RandomResizedCrop(384,scale=(0.5, 1.0), interpolation=Image.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            RandomAugment(2,7,isPIL=True,augs=['Identity','AutoContrast','Equalize','Brightness','Sharpness',\n",
    "                                              'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Rotate']),     \n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])  \n",
    "\n",
    "vqa_test_dataset = vqa_dataset(['/home/xinghao/code/datasets/vqa2/vqa_train.json'], train_transform, '/home/xinghao/code/datasets/vqa2', '', split='train') \n",
    "print(vqa_test_dataset.__getitem__(1))\n",
    "train_loader = create_loader((vqa_test_dataset,),[None],\n",
    "                                              batch_size=[3],\n",
    "                                              num_workers=[4],is_trains=[True], \n",
    "                                              collate_fns=[vqa_collate_fn])\n",
    "\n",
    "metric_logger = MetricLogger(delimiter=\"  \")\n",
    "\n",
    "# for i,(image, question, answer, weights, n) in enumerate(train_loader[0]):\n",
    "#     print(image, question, answer, weights, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:f21ippje) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>a</td><td>▁▃▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>a</td><td>99</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sage-night-1</strong> at: <a href='https://wandb.ai/wenxuan_zhao/test/runs/f21ippje/workspace' target=\"_blank\">https://wandb.ai/wenxuan_zhao/test/runs/f21ippje/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240529_175243-f21ippje/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:f21ippje). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/admin1/5703-upload/5703/Transformer_VQA_no/wandb/run-20240529_175325-xs4s421i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wenxuan_zhao/test/runs/xs4s421i/workspace' target=\"_blank\">deft-river-2</a></strong> to <a href='https://wandb.ai/wenxuan_zhao/test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wenxuan_zhao/test' target=\"_blank\">https://wandb.ai/wenxuan_zhao/test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wenxuan_zhao/test/runs/xs4s421i/workspace' target=\"_blank\">https://wandb.ai/wenxuan_zhao/test/runs/xs4s421i/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init(\n",
    "    project=\"test\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"config\": 'config',\n",
    "    },\n",
    ")\n",
    "\n",
    "for i in range(100):\n",
    "    if (i+1)%25 == 0:\n",
    "        wandb.log({\n",
    "            'a':i/25\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'timm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpre_vqa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreVQA\n",
      "File \u001b[0;32m~/5703-upload/5703/Transformer_VQA_no/src/pre_vqa.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvision_transformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VisionTransformer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxbert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertConfig, BertModel, BertLMHeadModel\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/5703-upload/5703/Transformer_VQA_no/src/vision_transformer.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvision_transformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _cfg, PatchEmbed\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_model\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trunc_normal_, DropPath\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'timm'"
     ]
    }
   ],
   "source": [
    "from src.pre_vqa import PreVQA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fusion_encoder.embeddings.position_ids\n",
      "fusion_encoder.embeddings.word_embeddings.weight\n",
      "fusion_encoder.embeddings.position_embeddings.weight\n",
      "fusion_encoder.embeddings.token_type_embeddings.weight\n",
      "fusion_encoder.embeddings.LayerNorm.weight\n",
      "fusion_encoder.embeddings.LayerNorm.bias\n",
      "fusion_encoder.encoder.layer.0.attention.self.query.weight\n",
      "fusion_encoder.encoder.layer.0.attention.self.query.bias\n",
      "fusion_encoder.encoder.layer.0.attention.self.key.weight\n",
      "fusion_encoder.encoder.layer.0.attention.self.key.bias\n",
      "fusion_encoder.encoder.layer.0.attention.self.value.weight\n",
      "fusion_encoder.encoder.layer.0.attention.self.value.bias\n",
      "fusion_encoder.encoder.layer.0.attention.output.dense.weight\n",
      "fusion_encoder.encoder.layer.0.attention.output.dense.bias\n",
      "fusion_encoder.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "fusion_encoder.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "fusion_encoder.encoder.layer.0.crossattention.self.query.weight\n",
      "fusion_encoder.encoder.layer.0.crossattention.self.query.bias\n",
      "fusion_encoder.encoder.layer.0.crossattention.self.key.weight\n",
      "fusion_encoder.encoder.layer.0.crossattention.self.key.bias\n",
      "fusion_encoder.encoder.layer.0.crossattention.self.value.weight\n",
      "fusion_encoder.encoder.layer.0.crossattention.self.value.bias\n",
      "fusion_encoder.encoder.layer.0.crossattention.output.dense.weight\n",
      "fusion_encoder.encoder.layer.0.crossattention.output.dense.bias\n",
      "fusion_encoder.encoder.layer.0.crossattention.output.LayerNorm.weight\n",
      "fusion_encoder.encoder.layer.0.crossattention.output.LayerNorm.bias\n",
      "fusion_encoder.encoder.layer.0.intermediate.dense.weight\n",
      "fusion_encoder.encoder.layer.0.intermediate.dense.bias\n",
      "fusion_encoder.encoder.layer.0.output.dense.weight\n",
      "fusion_encoder.encoder.layer.0.output.dense.bias\n",
      "fusion_encoder.encoder.layer.0.output.LayerNorm.weight\n",
      "fusion_encoder.encoder.layer.0.output.LayerNorm.bias\n",
      "fusion_encoder.encoder.layer.1.attention.self.query.weight\n",
      "fusion_encoder.encoder.layer.1.attention.self.query.bias\n",
      "fusion_encoder.encoder.layer.1.attention.self.key.weight\n",
      "fusion_encoder.encoder.layer.1.attention.self.key.bias\n",
      "fusion_encoder.encoder.layer.1.attention.self.value.weight\n",
      "fusion_encoder.encoder.layer.1.attention.self.value.bias\n",
      "fusion_encoder.encoder.layer.1.attention.output.dense.weight\n",
      "fusion_encoder.encoder.layer.1.attention.output.dense.bias\n",
      "fusion_encoder.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "fusion_encoder.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "fusion_encoder.encoder.layer.1.crossattention.self.query.weight\n",
      "fusion_encoder.encoder.layer.1.crossattention.self.query.bias\n",
      "fusion_encoder.encoder.layer.1.crossattention.self.key.weight\n",
      "fusion_encoder.encoder.layer.1.crossattention.self.key.bias\n",
      "fusion_encoder.encoder.layer.1.crossattention.self.value.weight\n",
      "fusion_encoder.encoder.layer.1.crossattention.self.value.bias\n",
      "fusion_encoder.encoder.layer.1.crossattention.output.dense.weight\n",
      "fusion_encoder.encoder.layer.1.crossattention.output.dense.bias\n",
      "fusion_encoder.encoder.layer.1.crossattention.output.LayerNorm.weight\n",
      "fusion_encoder.encoder.layer.1.crossattention.output.LayerNorm.bias\n",
      "fusion_encoder.encoder.layer.1.intermediate.dense.weight\n",
      "fusion_encoder.encoder.layer.1.intermediate.dense.bias\n",
      "fusion_encoder.encoder.layer.1.output.dense.weight\n",
      "fusion_encoder.encoder.layer.1.output.dense.bias\n",
      "fusion_encoder.encoder.layer.1.output.LayerNorm.weight\n",
      "fusion_encoder.encoder.layer.1.output.LayerNorm.bias\n",
      "fusion_encoder.encoder.layer.2.attention.self.query.weight\n",
      "fusion_encoder.encoder.layer.2.attention.self.query.bias\n",
      "fusion_encoder.encoder.layer.2.attention.self.key.weight\n",
      "fusion_encoder.encoder.layer.2.attention.self.key.bias\n",
      "fusion_encoder.encoder.layer.2.attention.self.value.weight\n",
      "fusion_encoder.encoder.layer.2.attention.self.value.bias\n",
      "fusion_encoder.encoder.layer.2.attention.output.dense.weight\n",
      "fusion_encoder.encoder.layer.2.attention.output.dense.bias\n",
      "fusion_encoder.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "fusion_encoder.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "fusion_encoder.encoder.layer.2.crossattention.self.query.weight\n",
      "fusion_encoder.encoder.layer.2.crossattention.self.query.bias\n",
      "fusion_encoder.encoder.layer.2.crossattention.self.key.weight\n",
      "fusion_encoder.encoder.layer.2.crossattention.self.key.bias\n",
      "fusion_encoder.encoder.layer.2.crossattention.self.value.weight\n",
      "fusion_encoder.encoder.layer.2.crossattention.self.value.bias\n",
      "fusion_encoder.encoder.layer.2.crossattention.output.dense.weight\n",
      "fusion_encoder.encoder.layer.2.crossattention.output.dense.bias\n",
      "fusion_encoder.encoder.layer.2.crossattention.output.LayerNorm.weight\n",
      "fusion_encoder.encoder.layer.2.crossattention.output.LayerNorm.bias\n",
      "fusion_encoder.encoder.layer.2.intermediate.dense.weight\n",
      "fusion_encoder.encoder.layer.2.intermediate.dense.bias\n",
      "fusion_encoder.encoder.layer.2.output.dense.weight\n",
      "fusion_encoder.encoder.layer.2.output.dense.bias\n",
      "fusion_encoder.encoder.layer.2.output.LayerNorm.weight\n",
      "fusion_encoder.encoder.layer.2.output.LayerNorm.bias\n",
      "fusion_encoder.encoder.layer.3.attention.self.query.weight\n",
      "fusion_encoder.encoder.layer.3.attention.self.query.bias\n",
      "fusion_encoder.encoder.layer.3.attention.self.key.weight\n",
      "fusion_encoder.encoder.layer.3.attention.self.key.bias\n",
      "fusion_encoder.encoder.layer.3.attention.self.value.weight\n",
      "fusion_encoder.encoder.layer.3.attention.self.value.bias\n",
      "fusion_encoder.encoder.layer.3.attention.output.dense.weight\n",
      "fusion_encoder.encoder.layer.3.attention.output.dense.bias\n",
      "fusion_encoder.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "fusion_encoder.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "fusion_encoder.encoder.layer.3.crossattention.self.query.weight\n",
      "fusion_encoder.encoder.layer.3.crossattention.self.query.bias\n",
      "fusion_encoder.encoder.layer.3.crossattention.self.key.weight\n",
      "fusion_encoder.encoder.layer.3.crossattention.self.key.bias\n",
      "fusion_encoder.encoder.layer.3.crossattention.self.value.weight\n",
      "fusion_encoder.encoder.layer.3.crossattention.self.value.bias\n",
      "fusion_encoder.encoder.layer.3.crossattention.output.dense.weight\n",
      "fusion_encoder.encoder.layer.3.crossattention.output.dense.bias\n",
      "fusion_encoder.encoder.layer.3.crossattention.output.LayerNorm.weight\n",
      "fusion_encoder.encoder.layer.3.crossattention.output.LayerNorm.bias\n",
      "fusion_encoder.encoder.layer.3.intermediate.dense.weight\n",
      "fusion_encoder.encoder.layer.3.intermediate.dense.bias\n",
      "fusion_encoder.encoder.layer.3.output.dense.weight\n",
      "fusion_encoder.encoder.layer.3.output.dense.bias\n",
      "fusion_encoder.encoder.layer.3.output.LayerNorm.weight\n",
      "fusion_encoder.encoder.layer.3.output.LayerNorm.bias\n",
      "fusion_encoder.encoder.layer.4.attention.self.query.weight\n",
      "fusion_encoder.encoder.layer.4.attention.self.query.bias\n",
      "fusion_encoder.encoder.layer.4.attention.self.key.weight\n",
      "fusion_encoder.encoder.layer.4.attention.self.key.bias\n",
      "fusion_encoder.encoder.layer.4.attention.self.value.weight\n",
      "fusion_encoder.encoder.layer.4.attention.self.value.bias\n",
      "fusion_encoder.encoder.layer.4.attention.output.dense.weight\n",
      "fusion_encoder.encoder.layer.4.attention.output.dense.bias\n",
      "fusion_encoder.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "fusion_encoder.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "fusion_encoder.encoder.layer.4.crossattention.self.query.weight\n",
      "fusion_encoder.encoder.layer.4.crossattention.self.query.bias\n",
      "fusion_encoder.encoder.layer.4.crossattention.self.key.weight\n",
      "fusion_encoder.encoder.layer.4.crossattention.self.key.bias\n",
      "fusion_encoder.encoder.layer.4.crossattention.self.value.weight\n",
      "fusion_encoder.encoder.layer.4.crossattention.self.value.bias\n",
      "fusion_encoder.encoder.layer.4.crossattention.output.dense.weight\n",
      "fusion_encoder.encoder.layer.4.crossattention.output.dense.bias\n",
      "fusion_encoder.encoder.layer.4.crossattention.output.LayerNorm.weight\n",
      "fusion_encoder.encoder.layer.4.crossattention.output.LayerNorm.bias\n",
      "fusion_encoder.encoder.layer.4.intermediate.dense.weight\n",
      "fusion_encoder.encoder.layer.4.intermediate.dense.bias\n",
      "fusion_encoder.encoder.layer.4.output.dense.weight\n",
      "fusion_encoder.encoder.layer.4.output.dense.bias\n",
      "fusion_encoder.encoder.layer.4.output.LayerNorm.weight\n",
      "fusion_encoder.encoder.layer.4.output.LayerNorm.bias\n",
      "fusion_encoder.encoder.layer.5.attention.self.query.weight\n",
      "fusion_encoder.encoder.layer.5.attention.self.query.bias\n",
      "fusion_encoder.encoder.layer.5.attention.self.key.weight\n",
      "fusion_encoder.encoder.layer.5.attention.self.key.bias\n",
      "fusion_encoder.encoder.layer.5.attention.self.value.weight\n",
      "fusion_encoder.encoder.layer.5.attention.self.value.bias\n",
      "fusion_encoder.encoder.layer.5.attention.output.dense.weight\n",
      "fusion_encoder.encoder.layer.5.attention.output.dense.bias\n",
      "fusion_encoder.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "fusion_encoder.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "fusion_encoder.encoder.layer.5.crossattention.self.query.weight\n",
      "fusion_encoder.encoder.layer.5.crossattention.self.query.bias\n",
      "fusion_encoder.encoder.layer.5.crossattention.self.key.weight\n",
      "fusion_encoder.encoder.layer.5.crossattention.self.key.bias\n",
      "fusion_encoder.encoder.layer.5.crossattention.self.value.weight\n",
      "fusion_encoder.encoder.layer.5.crossattention.self.value.bias\n",
      "fusion_encoder.encoder.layer.5.crossattention.output.dense.weight\n",
      "fusion_encoder.encoder.layer.5.crossattention.output.dense.bias\n",
      "fusion_encoder.encoder.layer.5.crossattention.output.LayerNorm.weight\n",
      "fusion_encoder.encoder.layer.5.crossattention.output.LayerNorm.bias\n",
      "fusion_encoder.encoder.layer.5.intermediate.dense.weight\n",
      "fusion_encoder.encoder.layer.5.intermediate.dense.bias\n",
      "fusion_encoder.encoder.layer.5.output.dense.weight\n",
      "fusion_encoder.encoder.layer.5.output.dense.bias\n",
      "fusion_encoder.encoder.layer.5.output.LayerNorm.weight\n",
      "fusion_encoder.encoder.layer.5.output.LayerNorm.bias\n",
      "fusion_encoder.cls.predictions.bias\n",
      "fusion_encoder.cls.predictions.transform.dense.weight\n",
      "fusion_encoder.cls.predictions.transform.dense.bias\n",
      "fusion_encoder.cls.predictions.transform.LayerNorm.weight\n",
      "fusion_encoder.cls.predictions.transform.LayerNorm.bias\n",
      "fusion_encoder.cls.predictions.decoder.weight\n",
      "fusion_encoder.cls.predictions.decoder.bias\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "state_dict = torch.load('/home/admin1/5703-upload/5703/Transformer_VQA/ALBEF.pth', map_location='cpu')['model']\n",
    "for key in list(state_dict.keys()):\n",
    "    \n",
    "    if 'text_encoder' in key and 'text_encoder_m' not in key:\n",
    "        \n",
    "        if 'layer' in key:\n",
    "            encoder_keys = key.split('.')\n",
    "            layer_num = int(encoder_keys[4])\n",
    "            \n",
    "            if layer_num >= 6:\n",
    "                decoder_layer_num = (layer_num-6)\n",
    "                encoder_keys[4] = str(decoder_layer_num)\n",
    "                encoder_key = '.'.join(encoder_keys)\n",
    "        else:\n",
    "            encoder_key = key\n",
    "        if 'bert' in encoder_key:\n",
    "            encoder_key = encoder_key.replace('bert.','')\n",
    "        decoder_key = encoder_key.replace('text_encoder','fusion_encoder')\n",
    "        state_dict[decoder_key] = state_dict[key]\n",
    "    del state_dict[key]\n",
    "for key in list(state_dict.keys()):\n",
    "    print(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:true) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">true</strong> at: <a href='https://wandb.ai/wenxuan_zhao/pre_vqa/runs/true/workspace' target=\"_blank\">https://wandb.ai/wenxuan_zhao/pre_vqa/runs/true/workspace</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240518_073242-true/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:true). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/admin1/5703-upload/5703/Transformer_VQA_no/wandb/run-20240518_073648-true</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/wenxuan_zhao/pre_vqa/runs/true/workspace' target=\"_blank\">true</a></strong> to <a href='https://wandb.ai/wenxuan_zhao/pre_vqa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wenxuan_zhao/pre_vqa' target=\"_blank\">https://wandb.ai/wenxuan_zhao/pre_vqa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wenxuan_zhao/pre_vqa/runs/true/workspace' target=\"_blank\">https://wandb.ai/wenxuan_zhao/pre_vqa/runs/true/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<wandb.sdk.wandb_run.Run object at 0x7f5825473ac0>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">true</strong> at: <a href='https://wandb.ai/wenxuan_zhao/pre_vqa/runs/true/workspace' target=\"_blank\">https://wandb.ai/wenxuan_zhao/pre_vqa/runs/true/workspace</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240518_073648-true/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    " \n",
    "# 假设你已经在W&B中创建了一个实验，并且已经上传了一个名为 'my_table' 的表格数据\n",
    "# 你可以通过以下代码来获取这个表格数据：\n",
    " \n",
    "run = wandb.init(project=\"pre_vqa\", resume=\"true\")  # 如果已经有了实验，可以通过resume来加载\n",
    " \n",
    "# 获取上传的表格数据\n",
    "run.\n",
    " \n",
    "# 打印表格数据\n",
    "print(run)\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
