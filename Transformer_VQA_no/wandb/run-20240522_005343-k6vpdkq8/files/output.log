Not using distributed mode
Creating vqa datasets
Creating model
Cosine annealing scheduler will have no effect on the learning rate since t_initial = t_mul = eta_mul = 1.
load checkpoint from /home/admin1/5703-upload/5703/Transformer_VQA_no/8-epoch.pth and ALBEF.fusion ##new , fuse the img and pseduo prompt after inversion
_IncompatibleKeys(missing_keys=['inversion.0.weight', 'inversion.0.bias', 'inversion.2.weight', 'inversion.2.bias', 'inversion.4.weight', 'inversion.4.bias'], unexpected_keys=['fusion_encoder.cls.predictions.bias', 'fusion_encoder.cls.predictions.transform.dense.weight', 'fusion_encoder.cls.predictions.transform.dense.bias', 'fusion_encoder.cls.predictions.transform.LayerNorm.weight', 'fusion_encoder.cls.predictions.transform.LayerNorm.bias', 'fusion_encoder.cls.predictions.decoder.weight', 'fusion_encoder.cls.predictions.decoder.bias'])
Start training
Train Epoch: [0]  [  0/563]  eta: 0:22:12  lr: 0.000010  loss: 0.6989  time: 2.3668  data: 0.9699  max mem: 11149
Train Epoch: [0]  [ 50/563]  eta: 0:05:11  lr: 0.000010  loss: 0.4934  time: 0.5717  data: 0.0001  max mem: 14771
Train Epoch: [0]  [100/563]  eta: 0:04:32  lr: 0.000010  loss: 0.3030  time: 0.5662  data: 0.0001  max mem: 14771
Train Epoch: [0]  [150/563]  eta: 0:04:00  lr: 0.000013  loss: 0.2169  time: 0.5575  data: 0.0001  max mem: 14771
Train Epoch: [0]  [200/563]  eta: 0:03:29  lr: 0.000013  loss: 0.1698  time: 0.5635  data: 0.0001  max mem: 14771
Train Epoch: [0]  [250/563]  eta: 0:02:59  lr: 0.000015  loss: 0.3805  time: 0.5635  data: 0.0001  max mem: 14771
Train Epoch: [0]  [300/563]  eta: 0:02:30  lr: 0.000015  loss: 0.2099  time: 0.5689  data: 0.0001  max mem: 14771
Train Epoch: [0]  [350/563]  eta: 0:02:01  lr: 0.000018  loss: 0.2123  time: 0.5629  data: 0.0001  max mem: 14771
Train Epoch: [0]  [400/563]  eta: 0:01:32  lr: 0.000018  loss: 0.1549  time: 0.5632  data: 0.0001  max mem: 14771
Train Epoch: [0]  [450/563]  eta: 0:01:04  lr: 0.000020  loss: 0.1028  time: 0.5676  data: 0.0001  max mem: 14771
Train Epoch: [0]  [500/563]  eta: 0:00:35  lr: 0.000020  loss: 0.0906  time: 0.5606  data: 0.0001  max mem: 14773
Train Epoch: [0]  [550/563]  eta: 0:00:07  lr: 0.000020  loss: 0.1812  time: 0.5667  data: 0.0001  max mem: 14773
Train Epoch: [0]  [562/563]  eta: 0:00:00  lr: 0.000020  loss: 0.0835  time: 0.5531  data: 0.0001  max mem: 14773
Train Epoch: [0] Total time: 0:05:20 (0.5690 s / it)
Averaged stats: lr: 0.0000  loss: 0.2908
Generate VQA test result:  [  0/282]  eta: 0:03:39    time: 0.7794  data: 0.3227  max mem: 14773
Generate VQA test result:  [ 50/282]  eta: 0:00:49    time: 0.2033  data: 0.0001  max mem: 14773
Generate VQA test result:  [100/282]  eta: 0:00:38    time: 0.2032  data: 0.0001  max mem: 14773
Generate VQA test result:  [150/282]  eta: 0:00:27    time: 0.2043  data: 0.0001  max mem: 14773
Generate VQA test result:  [200/282]  eta: 0:00:16    time: 0.2043  data: 0.0001  max mem: 14773
Generate VQA test result:  [250/282]  eta: 0:00:06    time: 0.2039  data: 0.0001  max mem: 14773
Generate VQA test result:  [281/282]  eta: 0:00:00    time: 0.2027  data: 0.0001  max mem: 14773
Generate VQA test result: Total time: 0:00:58 (0.2059 s / it)
0.8466312056737588
Generate VQA test result:  [  0/282]  eta: 0:02:36    time: 0.5557  data: 0.3228  max mem: 14773
Generate VQA test result:  [ 50/282]  eta: 0:00:48    time: 0.2048  data: 0.0001  max mem: 14773
Generate VQA test result:  [100/282]  eta: 0:00:37    time: 0.2043  data: 0.0001  max mem: 14773
Generate VQA test result:  [150/282]  eta: 0:00:27    time: 0.2043  data: 0.0001  max mem: 14773
Generate VQA test result:  [200/282]  eta: 0:00:16    time: 0.2040  data: 0.0001  max mem: 14773
Generate VQA test result:  [250/282]  eta: 0:00:06    time: 0.2048  data: 0.0001  max mem: 14773
Generate VQA test result:  [281/282]  eta: 0:00:00    time: 0.2035  data: 0.0001  max mem: 14773
Generate VQA test result: Total time: 0:00:58 (0.2059 s / it)
result file saved to output/vqa/result/vqa_result_epoch0.json
Training time 0:07:22