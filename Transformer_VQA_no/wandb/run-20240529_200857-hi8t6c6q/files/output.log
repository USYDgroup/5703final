Not using distributed mode
Creating vqa datasets
Creating model
reshape position embedding from 256 to 576
text_encoder.bert.embeddings.position_ids
text_encoder.bert.embeddings.word_embeddings.weight
text_encoder.bert.embeddings.position_embeddings.weight
text_encoder.bert.embeddings.token_type_embeddings.weight
text_encoder.bert.embeddings.LayerNorm.weight
text_encoder.bert.embeddings.LayerNorm.bias
text_encoder.cls.predictions.bias
text_encoder.cls.predictions.transform.dense.weight
text_encoder.cls.predictions.transform.dense.bias
text_encoder.cls.predictions.transform.LayerNorm.weight
text_encoder.cls.predictions.transform.LayerNorm.bias
text_encoder.cls.predictions.decoder.weight
text_encoder.cls.predictions.decoder.bias
load checkpoint from /home/admin1/5703-upload/5703/Transformer_VQA_no/ALBEF.pth
_IncompatibleKeys(missing_keys=['inversion.0.weight', 'inversion.0.bias', 'inversion.2.weight', 'inversion.2.bias', 'inversion.4.weight', 'inversion.4.bias', 'fusion_encoder.embeddings.position_ids', 'fusion_encoder.embeddings.word_embeddings.weight', 'fusion_encoder.embeddings.position_embeddings.weight', 'fusion_encoder.embeddings.token_type_embeddings.weight', 'fusion_encoder.embeddings.LayerNorm.weight', 'fusion_encoder.embeddings.LayerNorm.bias'], unexpected_keys=['temp', 'image_queue', 'text_queue', 'queue_ptr', 'vision_proj.weight', 'vision_proj.bias', 'text_proj.weight', 'text_proj.bias', 'itm_head.weight', 'itm_head.bias', 'vision_proj_m.weight', 'vision_proj_m.bias', 'text_proj_m.weight', 'text_proj_m.bias'])
load checkpoint from /home/admin1/5703-upload/5703/Transformer_VQA_no/ALBEF.pth and ALBEF.fusion ##new , fuse the img and pseduo prompt after inversion
_IncompatibleKeys(missing_keys=['inversion.0.weight', 'inversion.0.bias', 'inversion.2.weight', 'inversion.2.bias', 'inversion.4.weight', 'inversion.4.bias', 'fusion_encoder.embeddings.position_ids', 'fusion_encoder.embeddings.word_embeddings.weight', 'fusion_encoder.embeddings.position_embeddings.weight', 'fusion_encoder.embeddings.token_type_embeddings.weight', 'fusion_encoder.embeddings.LayerNorm.weight', 'fusion_encoder.embeddings.LayerNorm.bias'], unexpected_keys=['temp', 'image_queue', 'text_queue', 'queue_ptr', 'vision_proj.weight', 'vision_proj.bias', 'text_proj.weight', 'text_proj.bias', 'itm_head.weight', 'itm_head.bias', 'vision_proj_m.weight', 'vision_proj_m.bias', 'text_proj_m.weight', 'text_proj_m.bias'])
Start training
Traceback (most recent call last):
  File "/home/admin1/5703-upload/5703/Transformer_VQA_no/run_vqa2.py", line 385, in <module>
    main(args, config)
  File "/home/admin1/5703-upload/5703/Transformer_VQA_no/run_vqa2.py", line 315, in main
    train_stats = train(model, train_loader, optimizer, tokenizer, epoch, warmup_steps, device, lr_scheduler, config)
  File "/home/admin1/5703-upload/5703/Transformer_VQA_no/run_vqa2.py", line 77, in train
    for i,(image, question, answer, weights, n) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):
  File "/home/admin1/5703-upload/5703/Transformer_VQA_no/utils.py", line 137, in log_every
    for obj in iterable:
  File "/home/admin1/anconda3/envs/pytorch2.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 633, in __next__
    data = self._next_data()
  File "/home/admin1/anconda3/envs/pytorch2.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1328, in _next_data
    idx, data = self._get_data()
  File "/home/admin1/anconda3/envs/pytorch2.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1284, in _get_data
    success, data = self._try_get_data()
  File "/home/admin1/anconda3/envs/pytorch2.0/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1132, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/admin1/anconda3/envs/pytorch2.0/lib/python3.9/queue.py", line 180, in get
    self.not_empty.wait(remaining)
  File "/home/admin1/anconda3/envs/pytorch2.0/lib/python3.9/threading.py", line 316, in wait
    gotit = waiter.acquire(True, timeout)
KeyboardInterrupt