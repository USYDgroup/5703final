Not using distributed mode
Creating vqa datasets
Creating model
reshape position embedding from 256 to 576
load checkpoint from /home/admin1/5703-upload/5703/Transformer_VQA_no/ALBEF.pth and ALBEF.fusion ##new , fuse the img and pseduo prompt after inversion
_IncompatibleKeys(missing_keys=['inversion.0.weight', 'inversion.0.bias', 'inversion.2.weight', 'inversion.2.bias', 'inversion.4.weight', 'inversion.4.bias'], unexpected_keys=['temp', 'image_queue', 'text_queue', 'queue_ptr', 'vision_proj.weight', 'vision_proj.bias', 'text_proj.weight', 'text_proj.bias', 'itm_head.weight', 'itm_head.bias', 'visual_encoder_m.cls_token', 'visual_encoder_m.pos_embed', 'visual_encoder_m.patch_embed.proj.weight', 'visual_encoder_m.patch_embed.proj.bias', 'visual_encoder_m.blocks.0.norm1.weight', 'visual_encoder_m.blocks.0.norm1.bias', 'visual_encoder_m.blocks.0.attn.qkv.weight', 'visual_encoder_m.blocks.0.attn.qkv.bias', 'visual_encoder_m.blocks.0.attn.proj.weight', 'visual_encoder_m.blocks.0.attn.proj.bias', 'visual_encoder_m.blocks.0.norm2.weight', 'visual_encoder_m.blocks.0.norm2.bias', 'visual_encoder_m.blocks.0.mlp.fc1.weight', 'visual_encoder_m.blocks.0.mlp.fc1.bias', 'visual_encoder_m.blocks.0.mlp.fc2.weight', 'visual_encoder_m.blocks.0.mlp.fc2.bias', 'visual_encoder_m.blocks.1.norm1.weight', 'visual_encoder_m.blocks.1.norm1.bias', 'visual_encoder_m.blocks.1.attn.qkv.weight', 'visual_encoder_m.blocks.1.attn.qkv.bias', 'visual_encoder_m.blocks.1.attn.proj.weight', 'visual_encoder_m.blocks.1.attn.proj.bias', 'visual_encoder_m.blocks.1.norm2.weight', 'visual_encoder_m.blocks.1.norm2.bias', 'visual_encoder_m.blocks.1.mlp.fc1.weight', 'visual_encoder_m.blocks.1.mlp.fc1.bias', 'visual_encoder_m.blocks.1.mlp.fc2.weight', 'visual_encoder_m.blocks.1.mlp.fc2.bias', 'visual_encoder_m.blocks.2.norm1.weight', 'visual_encoder_m.blocks.2.norm1.bias', 'visual_encoder_m.blocks.2.attn.qkv.weight', 'visual_encoder_m.blocks.2.attn.qkv.bias', 'visual_encoder_m.blocks.2.attn.proj.weight', 'visual_encoder_m.blocks.2.attn.proj.bias', 'visual_encoder_m.blocks.2.norm2.weight', 'visual_encoder_m.blocks.2.norm2.bias', 'visual_encoder_m.blocks.2.mlp.fc1.weight', 'visual_encoder_m.blocks.2.mlp.fc1.bias', 'visual_encoder_m.blocks.2.mlp.fc2.weight', 'visual_encoder_m.blocks.2.mlp.fc2.bias', 'visual_encoder_m.blocks.3.norm1.weight', 'visual_encoder_m.blocks.3.norm1.bias', 'visual_encoder_m.blocks.3.attn.qkv.weight', 'visual_encoder_m.blocks.3.attn.qkv.bias', 'visual_encoder_m.blocks.3.attn.proj.weight', 'visual_encoder_m.blocks.3.attn.proj.bias', 'visual_encoder_m.blocks.3.norm2.weight', 'visual_encoder_m.blocks.3.norm2.bias', 'visual_encoder_m.blocks.3.mlp.fc1.weight', 'visual_encoder_m.blocks.3.mlp.fc1.bias', 'visual_encoder_m.blocks.3.mlp.fc2.weight', 'visual_encoder_m.blocks.3.mlp.fc2.bias', 'visual_encoder_m.blocks.4.norm1.weight', 'visual_encoder_m.blocks.4.norm1.bias', 'visual_encoder_m.blocks.4.attn.qkv.weight', 'visual_encoder_m.blocks.4.attn.qkv.bias', 'visual_encoder_m.blocks.4.attn.proj.weight', 'visual_encoder_m.blocks.4.attn.proj.bias', 'visual_encoder_m.blocks.4.norm2.weight', 'visual_encoder_m.blocks.4.norm2.bias', 'visual_encoder_m.blocks.4.mlp.fc1.weight', 'visual_encoder_m.blocks.4.mlp.fc1.bias', 'visual_encoder_m.blocks.4.mlp.fc2.weight', 'visual_encoder_m.blocks.4.mlp.fc2.bias', 'visual_encoder_m.blocks.5.norm1.weight', 'visual_encoder_m.blocks.5.norm1.bias', 'visual_encoder_m.blocks.5.attn.qkv.weight', 'visual_encoder_m.blocks.5.attn.qkv.bias', 'visual_encoder_m.blocks.5.attn.proj.weight', 'visual_encoder_m.blocks.5.attn.proj.bias', 'visual_encoder_m.blocks.5.norm2.weight', 'visual_encoder_m.blocks.5.norm2.bias', 'visual_encoder_m.blocks.5.mlp.fc1.weight', 'visual_encoder_m.blocks.5.mlp.fc1.bias', 'visual_encoder_m.blocks.5.mlp.fc2.weight', 'visual_encoder_m.blocks.5.mlp.fc2.bias', 'visual_encoder_m.blocks.6.norm1.weight', 'visual_encoder_m.blocks.6.norm1.bias', 'visual_encoder_m.blocks.6.attn.qkv.weight', 'visual_encoder_m.blocks.6.attn.qkv.bias', 'visual_encoder_m.blocks.6.attn.proj.weight', 'visual_encoder_m.blocks.6.attn.proj.bias', 'visual_encoder_m.blocks.6.norm2.weight', 'visual_encoder_m.blocks.6.norm2.bias', 'visual_encoder_m.blocks.6.mlp.fc1.weight', 'visual_encoder_m.blocks.6.mlp.fc1.bias', 'visual_encoder_m.blocks.6.mlp.fc2.weight', 'visual_encoder_m.blocks.6.mlp.fc2.bias', 'visual_encoder_m.blocks.7.norm1.weight', 'visual_encoder_m.blocks.7.norm1.bias', 'visual_encoder_m.blocks.7.attn.qkv.weight', 'visual_encoder_m.blocks.7.attn.qkv.bias', 'visual_encoder_m.blocks.7.attn.proj.weight', 'visual_encoder_m.blocks.7.attn.proj.bias', 'visual_encoder_m.blocks.7.norm2.weight', 'visual_encoder_m.blocks.7.norm2.bias', 'visual_encoder_m.blocks.7.mlp.fc1.weight', 'visual_encoder_m.blocks.7.mlp.fc1.bias', 'visual_encoder_m.blocks.7.mlp.fc2.weight', 'visual_encoder_m.blocks.7.mlp.fc2.bias', 'visual_encoder_m.blocks.8.norm1.weight', 'visual_encoder_m.blocks.8.norm1.bias', 'visual_encoder_m.blocks.8.attn.qkv.weight', 'visual_encoder_m.blocks.8.attn.qkv.bias', 'visual_encoder_m.blocks.8.attn.proj.weight', 'visual_encoder_m.blocks.8.attn.proj.bias', 'visual_encoder_m.blocks.8.norm2.weight', 'visual_encoder_m.blocks.8.norm2.bias', 'visual_encoder_m.blocks.8.mlp.fc1.weight', 'visual_encoder_m.blocks.8.mlp.fc1.bias', 'visual_encoder_m.blocks.8.mlp.fc2.weight', 'visual_encoder_m.blocks.8.mlp.fc2.bias', 'visual_encoder_m.blocks.9.norm1.weight', 'visual_encoder_m.blocks.9.norm1.bias', 'visual_encoder_m.blocks.9.attn.qkv.weight', 'visual_encoder_m.blocks.9.attn.qkv.bias', 'visual_encoder_m.blocks.9.attn.proj.weight', 'visual_encoder_m.blocks.9.attn.proj.bias', 'visual_encoder_m.blocks.9.norm2.weight', 'visual_encoder_m.blocks.9.norm2.bias', 'visual_encoder_m.blocks.9.mlp.fc1.weight', 'visual_encoder_m.blocks.9.mlp.fc1.bias', 'visual_encoder_m.blocks.9.mlp.fc2.weight', 'visual_encoder_m.blocks.9.mlp.fc2.bias', 'visual_encoder_m.blocks.10.norm1.weight', 'visual_encoder_m.blocks.10.norm1.bias', 'visual_encoder_m.blocks.10.attn.qkv.weight', 'visual_encoder_m.blocks.10.attn.qkv.bias', 'visual_encoder_m.blocks.10.attn.proj.weight', 'visual_encoder_m.blocks.10.attn.proj.bias', 'visual_encoder_m.blocks.10.norm2.weight', 'visual_encoder_m.blocks.10.norm2.bias', 'visual_encoder_m.blocks.10.mlp.fc1.weight', 'visual_encoder_m.blocks.10.mlp.fc1.bias', 'visual_encoder_m.blocks.10.mlp.fc2.weight', 'visual_encoder_m.blocks.10.mlp.fc2.bias', 'visual_encoder_m.blocks.11.norm1.weight', 'visual_encoder_m.blocks.11.norm1.bias', 'visual_encoder_m.blocks.11.attn.qkv.weight', 'visual_encoder_m.blocks.11.attn.qkv.bias', 'visual_encoder_m.blocks.11.attn.proj.weight', 'visual_encoder_m.blocks.11.attn.proj.bias', 'visual_encoder_m.blocks.11.norm2.weight', 'visual_encoder_m.blocks.11.norm2.bias', 'visual_encoder_m.blocks.11.mlp.fc1.weight', 'visual_encoder_m.blocks.11.mlp.fc1.bias', 'visual_encoder_m.blocks.11.mlp.fc2.weight', 'visual_encoder_m.blocks.11.mlp.fc2.bias', 'visual_encoder_m.norm.weight', 'visual_encoder_m.norm.bias', 'vision_proj_m.weight', 'vision_proj_m.bias', 'text_proj_m.weight', 'text_proj_m.bias', 'text_encoder_m.embeddings.position_ids', 'text_decoder_m.bert.embeddings.position_ids', 'text_encoder_m.embeddings.word_embeddings.weight', 'text_decoder_m.bert.embeddings.word_embeddings.weight', 'text_encoder_m.embeddings.position_embeddings.weight', 'text_decoder_m.bert.embeddings.position_embeddings.weight', 'text_encoder_m.embeddings.token_type_embeddings.weight', 'text_decoder_m.bert.embeddings.token_type_embeddings.weight', 'text_encoder_m.embeddings.LayerNorm.weight', 'text_decoder_m.bert.embeddings.LayerNorm.weight', 'text_encoder_m.embeddings.LayerNorm.bias', 'text_decoder_m.bert.embeddings.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.attention.self.query.weight', 'text_encoder_m.encoder.layer.0.attention.self.query.bias', 'text_encoder_m.encoder.layer.0.attention.self.key.weight', 'text_encoder_m.encoder.layer.0.attention.self.key.bias', 'text_encoder_m.encoder.layer.0.attention.self.value.weight', 'text_encoder_m.encoder.layer.0.attention.self.value.bias', 'text_encoder_m.encoder.layer.0.attention.output.dense.weight', 'text_encoder_m.encoder.layer.0.attention.output.dense.bias', 'text_encoder_m.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.intermediate.dense.weight', 'text_encoder_m.encoder.layer.0.intermediate.dense.bias', 'text_encoder_m.encoder.layer.0.output.dense.weight', 'text_encoder_m.encoder.layer.0.output.dense.bias', 'text_encoder_m.encoder.layer.0.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.attention.self.query.weight', 'text_encoder_m.encoder.layer.1.attention.self.query.bias', 'text_encoder_m.encoder.layer.1.attention.self.key.weight', 'text_encoder_m.encoder.layer.1.attention.self.key.bias', 'text_encoder_m.encoder.layer.1.attention.self.value.weight', 'text_encoder_m.encoder.layer.1.attention.self.value.bias', 'text_encoder_m.encoder.layer.1.attention.output.dense.weight', 'text_encoder_m.encoder.layer.1.attention.output.dense.bias', 'text_encoder_m.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.intermediate.dense.weight', 'text_encoder_m.encoder.layer.1.intermediate.dense.bias', 'text_encoder_m.encoder.layer.1.output.dense.weight', 'text_encoder_m.encoder.layer.1.output.dense.bias', 'text_encoder_m.encoder.layer.1.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.attention.self.query.weight', 'text_encoder_m.encoder.layer.2.attention.self.query.bias', 'text_encoder_m.encoder.layer.2.attention.self.key.weight', 'text_encoder_m.encoder.layer.2.attention.self.key.bias', 'text_encoder_m.encoder.layer.2.attention.self.value.weight', 'text_encoder_m.encoder.layer.2.attention.self.value.bias', 'text_encoder_m.encoder.layer.2.attention.output.dense.weight', 'text_encoder_m.encoder.layer.2.attention.output.dense.bias', 'text_encoder_m.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.intermediate.dense.weight', 'text_encoder_m.encoder.layer.2.intermediate.dense.bias', 'text_encoder_m.encoder.layer.2.output.dense.weight', 'text_encoder_m.encoder.layer.2.output.dense.bias', 'text_encoder_m.encoder.layer.2.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.attention.self.query.weight', 'text_encoder_m.encoder.layer.3.attention.self.query.bias', 'text_encoder_m.encoder.layer.3.attention.self.key.weight', 'text_encoder_m.encoder.layer.3.attention.self.key.bias', 'text_encoder_m.encoder.layer.3.attention.self.value.weight', 'text_encoder_m.encoder.layer.3.attention.self.value.bias', 'text_encoder_m.encoder.layer.3.attention.output.dense.weight', 'text_encoder_m.encoder.layer.3.attention.output.dense.bias', 'text_encoder_m.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.intermediate.dense.weight', 'text_encoder_m.encoder.layer.3.intermediate.dense.bias', 'text_encoder_m.encoder.layer.3.output.dense.weight', 'text_encoder_m.encoder.layer.3.output.dense.bias', 'text_encoder_m.encoder.layer.3.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.attention.self.query.weight', 'text_encoder_m.encoder.layer.4.attention.self.query.bias', 'text_encoder_m.encoder.layer.4.attention.self.key.weight', 'text_encoder_m.encoder.layer.4.attention.self.key.bias', 'text_encoder_m.encoder.layer.4.attention.self.value.weight', 'text_encoder_m.encoder.layer.4.attention.self.value.bias', 'text_encoder_m.encoder.layer.4.attention.output.dense.weight', 'text_encoder_m.encoder.layer.4.attention.output.dense.bias', 'text_encoder_m.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.intermediate.dense.weight', 'text_encoder_m.encoder.layer.4.intermediate.dense.bias', 'text_encoder_m.encoder.layer.4.output.dense.weight', 'text_encoder_m.encoder.layer.4.output.dense.bias', 'text_encoder_m.encoder.layer.4.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.attention.self.query.weight', 'text_encoder_m.encoder.layer.5.attention.self.query.bias', 'text_encoder_m.encoder.layer.5.attention.self.key.weight', 'text_encoder_m.encoder.layer.5.attention.self.key.bias', 'text_encoder_m.encoder.layer.5.attention.self.value.weight', 'text_encoder_m.encoder.layer.5.attention.self.value.bias', 'text_encoder_m.encoder.layer.5.attention.output.dense.weight', 'text_encoder_m.encoder.layer.5.attention.output.dense.bias', 'text_encoder_m.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.intermediate.dense.weight', 'text_encoder_m.encoder.layer.5.intermediate.dense.bias', 'text_encoder_m.encoder.layer.5.output.dense.weight', 'text_encoder_m.encoder.layer.5.output.dense.bias', 'text_encoder_m.encoder.layer.5.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.attention.self.query.weight', 'text_decoder_m.bert.encoder.layer.0.attention.self.query.weight', 'text_encoder_m.encoder.layer.6.attention.self.query.bias', 'text_decoder_m.bert.encoder.layer.0.attention.self.query.bias', 'text_encoder_m.encoder.layer.6.attention.self.key.weight', 'text_decoder_m.bert.encoder.layer.0.attention.self.key.weight', 'text_encoder_m.encoder.layer.6.attention.self.key.bias', 'text_decoder_m.bert.encoder.layer.0.attention.self.key.bias', 'text_encoder_m.encoder.layer.6.attention.self.value.weight', 'text_decoder_m.bert.encoder.layer.0.attention.self.value.weight', 'text_encoder_m.encoder.layer.6.attention.self.value.bias', 'text_decoder_m.bert.encoder.layer.0.attention.self.value.bias', 'text_encoder_m.encoder.layer.6.attention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.0.attention.output.dense.weight', 'text_encoder_m.encoder.layer.6.attention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.0.attention.output.dense.bias', 'text_encoder_m.encoder.layer.6.attention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.attention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.query.weight', 'text_decoder_m.bert.encoder.layer.0.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.query.bias', 'text_decoder_m.bert.encoder.layer.0.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.key.weight', 'text_decoder_m.bert.encoder.layer.0.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.key.bias', 'text_decoder_m.bert.encoder.layer.0.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.value.weight', 'text_decoder_m.bert.encoder.layer.0.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.value.bias', 'text_decoder_m.bert.encoder.layer.0.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.6.crossattention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.0.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.6.crossattention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.0.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.intermediate.dense.weight', 'text_decoder_m.bert.encoder.layer.0.intermediate.dense.weight', 'text_encoder_m.encoder.layer.6.intermediate.dense.bias', 'text_decoder_m.bert.encoder.layer.0.intermediate.dense.bias', 'text_encoder_m.encoder.layer.6.output.dense.weight', 'text_decoder_m.bert.encoder.layer.0.output.dense.weight', 'text_encoder_m.encoder.layer.6.output.dense.bias', 'text_decoder_m.bert.encoder.layer.0.output.dense.bias', 'text_encoder_m.encoder.layer.6.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.0.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.0.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.attention.self.query.weight', 'text_decoder_m.bert.encoder.layer.1.attention.self.query.weight', 'text_encoder_m.encoder.layer.7.attention.self.query.bias', 'text_decoder_m.bert.encoder.layer.1.attention.self.query.bias', 'text_encoder_m.encoder.layer.7.attention.self.key.weight', 'text_decoder_m.bert.encoder.layer.1.attention.self.key.weight', 'text_encoder_m.encoder.layer.7.attention.self.key.bias', 'text_decoder_m.bert.encoder.layer.1.attention.self.key.bias', 'text_encoder_m.encoder.layer.7.attention.self.value.weight', 'text_decoder_m.bert.encoder.layer.1.attention.self.value.weight', 'text_encoder_m.encoder.layer.7.attention.self.value.bias', 'text_decoder_m.bert.encoder.layer.1.attention.self.value.bias', 'text_encoder_m.encoder.layer.7.attention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.1.attention.output.dense.weight', 'text_encoder_m.encoder.layer.7.attention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.1.attention.output.dense.bias', 'text_encoder_m.encoder.layer.7.attention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.attention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.query.weight', 'text_decoder_m.bert.encoder.layer.1.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.query.bias', 'text_decoder_m.bert.encoder.layer.1.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.key.weight', 'text_decoder_m.bert.encoder.layer.1.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.key.bias', 'text_decoder_m.bert.encoder.layer.1.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.value.weight', 'text_decoder_m.bert.encoder.layer.1.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.value.bias', 'text_decoder_m.bert.encoder.layer.1.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.7.crossattention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.1.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.7.crossattention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.1.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.intermediate.dense.weight', 'text_decoder_m.bert.encoder.layer.1.intermediate.dense.weight', 'text_encoder_m.encoder.layer.7.intermediate.dense.bias', 'text_decoder_m.bert.encoder.layer.1.intermediate.dense.bias', 'text_encoder_m.encoder.layer.7.output.dense.weight', 'text_decoder_m.bert.encoder.layer.1.output.dense.weight', 'text_encoder_m.encoder.layer.7.output.dense.bias', 'text_decoder_m.bert.encoder.layer.1.output.dense.bias', 'text_encoder_m.encoder.layer.7.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.1.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.1.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.attention.self.query.weight', 'text_decoder_m.bert.encoder.layer.2.attention.self.query.weight', 'text_encoder_m.encoder.layer.8.attention.self.query.bias', 'text_decoder_m.bert.encoder.layer.2.attention.self.query.bias', 'text_encoder_m.encoder.layer.8.attention.self.key.weight', 'text_decoder_m.bert.encoder.layer.2.attention.self.key.weight', 'text_encoder_m.encoder.layer.8.attention.self.key.bias', 'text_decoder_m.bert.encoder.layer.2.attention.self.key.bias', 'text_encoder_m.encoder.layer.8.attention.self.value.weight', 'text_decoder_m.bert.encoder.layer.2.attention.self.value.weight', 'text_encoder_m.encoder.layer.8.attention.self.value.bias', 'text_decoder_m.bert.encoder.layer.2.attention.self.value.bias', 'text_encoder_m.encoder.layer.8.attention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.2.attention.output.dense.weight', 'text_encoder_m.encoder.layer.8.attention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.2.attention.output.dense.bias', 'text_encoder_m.encoder.layer.8.attention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.attention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.query.weight', 'text_decoder_m.bert.encoder.layer.2.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.query.bias', 'text_decoder_m.bert.encoder.layer.2.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.key.weight', 'text_decoder_m.bert.encoder.layer.2.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.key.bias', 'text_decoder_m.bert.encoder.layer.2.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.value.weight', 'text_decoder_m.bert.encoder.layer.2.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.value.bias', 'text_decoder_m.bert.encoder.layer.2.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.8.crossattention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.2.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.8.crossattention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.2.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.intermediate.dense.weight', 'text_decoder_m.bert.encoder.layer.2.intermediate.dense.weight', 'text_encoder_m.encoder.layer.8.intermediate.dense.bias', 'text_decoder_m.bert.encoder.layer.2.intermediate.dense.bias', 'text_encoder_m.encoder.layer.8.output.dense.weight', 'text_decoder_m.bert.encoder.layer.2.output.dense.weight', 'text_encoder_m.encoder.layer.8.output.dense.bias', 'text_decoder_m.bert.encoder.layer.2.output.dense.bias', 'text_encoder_m.encoder.layer.8.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.2.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.2.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.attention.self.query.weight', 'text_decoder_m.bert.encoder.layer.3.attention.self.query.weight', 'text_encoder_m.encoder.layer.9.attention.self.query.bias', 'text_decoder_m.bert.encoder.layer.3.attention.self.query.bias', 'text_encoder_m.encoder.layer.9.attention.self.key.weight', 'text_decoder_m.bert.encoder.layer.3.attention.self.key.weight', 'text_encoder_m.encoder.layer.9.attention.self.key.bias', 'text_decoder_m.bert.encoder.layer.3.attention.self.key.bias', 'text_encoder_m.encoder.layer.9.attention.self.value.weight', 'text_decoder_m.bert.encoder.layer.3.attention.self.value.weight', 'text_encoder_m.encoder.layer.9.attention.self.value.bias', 'text_decoder_m.bert.encoder.layer.3.attention.self.value.bias', 'text_encoder_m.encoder.layer.9.attention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.3.attention.output.dense.weight', 'text_encoder_m.encoder.layer.9.attention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.3.attention.output.dense.bias', 'text_encoder_m.encoder.layer.9.attention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.attention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.query.weight', 'text_decoder_m.bert.encoder.layer.3.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.query.bias', 'text_decoder_m.bert.encoder.layer.3.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.key.weight', 'text_decoder_m.bert.encoder.layer.3.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.key.bias', 'text_decoder_m.bert.encoder.layer.3.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.value.weight', 'text_decoder_m.bert.encoder.layer.3.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.value.bias', 'text_decoder_m.bert.encoder.layer.3.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.9.crossattention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.3.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.9.crossattention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.3.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.intermediate.dense.weight', 'text_decoder_m.bert.encoder.layer.3.intermediate.dense.weight', 'text_encoder_m.encoder.layer.9.intermediate.dense.bias', 'text_decoder_m.bert.encoder.layer.3.intermediate.dense.bias', 'text_encoder_m.encoder.layer.9.output.dense.weight', 'text_decoder_m.bert.encoder.layer.3.output.dense.weight', 'text_encoder_m.encoder.layer.9.output.dense.bias', 'text_decoder_m.bert.encoder.layer.3.output.dense.bias', 'text_encoder_m.encoder.layer.9.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.3.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.3.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.attention.self.query.weight', 'text_decoder_m.bert.encoder.layer.4.attention.self.query.weight', 'text_encoder_m.encoder.layer.10.attention.self.query.bias', 'text_decoder_m.bert.encoder.layer.4.attention.self.query.bias', 'text_encoder_m.encoder.layer.10.attention.self.key.weight', 'text_decoder_m.bert.encoder.layer.4.attention.self.key.weight', 'text_encoder_m.encoder.layer.10.attention.self.key.bias', 'text_decoder_m.bert.encoder.layer.4.attention.self.key.bias', 'text_encoder_m.encoder.layer.10.attention.self.value.weight', 'text_decoder_m.bert.encoder.layer.4.attention.self.value.weight', 'text_encoder_m.encoder.layer.10.attention.self.value.bias', 'text_decoder_m.bert.encoder.layer.4.attention.self.value.bias', 'text_encoder_m.encoder.layer.10.attention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.4.attention.output.dense.weight', 'text_encoder_m.encoder.layer.10.attention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.4.attention.output.dense.bias', 'text_encoder_m.encoder.layer.10.attention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.attention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.query.weight', 'text_decoder_m.bert.encoder.layer.4.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.query.bias', 'text_decoder_m.bert.encoder.layer.4.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.key.weight', 'text_decoder_m.bert.encoder.layer.4.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.key.bias', 'text_decoder_m.bert.encoder.layer.4.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.value.weight', 'text_decoder_m.bert.encoder.layer.4.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.value.bias', 'text_decoder_m.bert.encoder.layer.4.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.10.crossattention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.4.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.10.crossattention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.4.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.intermediate.dense.weight', 'text_decoder_m.bert.encoder.layer.4.intermediate.dense.weight', 'text_encoder_m.encoder.layer.10.intermediate.dense.bias', 'text_decoder_m.bert.encoder.layer.4.intermediate.dense.bias', 'text_encoder_m.encoder.layer.10.output.dense.weight', 'text_decoder_m.bert.encoder.layer.4.output.dense.weight', 'text_encoder_m.encoder.layer.10.output.dense.bias', 'text_decoder_m.bert.encoder.layer.4.output.dense.bias', 'text_encoder_m.encoder.layer.10.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.4.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.4.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.attention.self.query.weight', 'text_decoder_m.bert.encoder.layer.5.attention.self.query.weight', 'text_encoder_m.encoder.layer.11.attention.self.query.bias', 'text_decoder_m.bert.encoder.layer.5.attention.self.query.bias', 'text_encoder_m.encoder.layer.11.attention.self.key.weight', 'text_decoder_m.bert.encoder.layer.5.attention.self.key.weight', 'text_encoder_m.encoder.layer.11.attention.self.key.bias', 'text_decoder_m.bert.encoder.layer.5.attention.self.key.bias', 'text_encoder_m.encoder.layer.11.attention.self.value.weight', 'text_decoder_m.bert.encoder.layer.5.attention.self.value.weight', 'text_encoder_m.encoder.layer.11.attention.self.value.bias', 'text_decoder_m.bert.encoder.layer.5.attention.self.value.bias', 'text_encoder_m.encoder.layer.11.attention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.5.attention.output.dense.weight', 'text_encoder_m.encoder.layer.11.attention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.5.attention.output.dense.bias', 'text_encoder_m.encoder.layer.11.attention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.attention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.query.weight', 'text_decoder_m.bert.encoder.layer.5.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.query.bias', 'text_decoder_m.bert.encoder.layer.5.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.key.weight', 'text_decoder_m.bert.encoder.layer.5.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.key.bias', 'text_decoder_m.bert.encoder.layer.5.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.value.weight', 'text_decoder_m.bert.encoder.layer.5.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.value.bias', 'text_decoder_m.bert.encoder.layer.5.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.11.crossattention.output.dense.weight', 'text_decoder_m.bert.encoder.layer.5.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.11.crossattention.output.dense.bias', 'text_decoder_m.bert.encoder.layer.5.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.intermediate.dense.weight', 'text_decoder_m.bert.encoder.layer.5.intermediate.dense.weight', 'text_encoder_m.encoder.layer.11.intermediate.dense.bias', 'text_decoder_m.bert.encoder.layer.5.intermediate.dense.bias', 'text_encoder_m.encoder.layer.11.output.dense.weight', 'text_decoder_m.bert.encoder.layer.5.output.dense.weight', 'text_encoder_m.encoder.layer.11.output.dense.bias', 'text_decoder_m.bert.encoder.layer.5.output.dense.bias', 'text_encoder_m.encoder.layer.11.output.LayerNorm.weight', 'text_decoder_m.bert.encoder.layer.5.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.output.LayerNorm.bias', 'text_decoder_m.bert.encoder.layer.5.output.LayerNorm.bias', 'text_decoder_m.cls.predictions.bias', 'text_decoder_m.cls.predictions.transform.dense.weight', 'text_decoder_m.cls.predictions.transform.dense.bias', 'text_decoder_m.cls.predictions.transform.LayerNorm.weight', 'text_decoder_m.cls.predictions.transform.LayerNorm.bias', 'text_decoder_m.cls.predictions.decoder.weight', 'text_decoder_m.cls.predictions.decoder.bias', 'fusion_encoder.cls.predictions.bias', 'fusion_encoder.cls.predictions.transform.dense.weight', 'fusion_encoder.cls.predictions.transform.dense.bias', 'fusion_encoder.cls.predictions.transform.LayerNorm.weight', 'fusion_encoder.cls.predictions.transform.LayerNorm.bias', 'fusion_encoder.cls.predictions.decoder.weight', 'fusion_encoder.cls.predictions.decoder.bias'])
Start training
Train Epoch: [0]  [    0/27734]  eta: 1 day, 1:13:03  lr: 0.000010  loss: 43.4484  time: 3.2734  data: 1.5074  max mem: 11191
Train Epoch: [0]  [   50/27734]  eta: 4:43:35  lr: 0.000010  loss: 10.0136  time: 0.5615  data: 0.0002  max mem: 15289
Train Epoch: [0]  [  100/27734]  eta: 4:31:56  lr: 0.000010  loss: 7.6969  time: 0.5663  data: 0.0002  max mem: 15289
Train Epoch: [0]  [  150/27734]  eta: 4:27:51  lr: 0.000013  loss: 9.2063  time: 0.5678  data: 0.0002  max mem: 15327
Train Epoch: [0]  [  200/27734]  eta: 4:24:58  lr: 0.000013  loss: 7.5088  time: 0.5612  data: 0.0002  max mem: 15327
Train Epoch: [0]  [  250/27734]  eta: 4:23:43  lr: 0.000015  loss: 4.8553  time: 0.5701  data: 0.0002  max mem: 15327
Train Epoch: [0]  [  300/27734]  eta: 4:22:31  lr: 0.000015  loss: 4.1139  time: 0.5636  data: 0.0003  max mem: 15327
Train Epoch: [0]  [  350/27734]  eta: 4:21:51  lr: 0.000018  loss: 7.1853  time: 0.5699  data: 0.0002  max mem: 15327
Train Epoch: [0]  [  400/27734]  eta: 4:21:56  lr: 0.000018  loss: 8.7441  time: 0.6011  data: 0.0002  max mem: 15327
Train Epoch: [0]  [  450/27734]  eta: 4:20:55  lr: 0.000020  loss: 8.2778  time: 0.5603  data: 0.0002  max mem: 15327
Train Epoch: [0]  [  500/27734]  eta: 4:20:11  lr: 0.000020  loss: 8.1015  time: 0.5659  data: 0.0002  max mem: 15337
Train Epoch: [0]  [  550/27734]  eta: 4:19:23  lr: 0.000020  loss: 5.9087  time: 0.5674  data: 0.0002  max mem: 15371
Train Epoch: [0]  [  600/27734]  eta: 4:19:02  lr: 0.000020  loss: 7.7153  time: 0.5745  data: 0.0002  max mem: 15371
Train Epoch: [0]  [  650/27734]  eta: 4:18:33  lr: 0.000020  loss: 5.5990  time: 0.5712  data: 0.0001  max mem: 15371
Train Epoch: [0]  [  700/27734]  eta: 4:17:42  lr: 0.000020  loss: 4.7033  time: 0.5580  data: 0.0002  max mem: 15371
Train Epoch: [0]  [  750/27734]  eta: 4:16:54  lr: 0.000020  loss: 5.1584  time: 0.5605  data: 0.0002  max mem: 15371
Train Epoch: [0]  [  800/27734]  eta: 4:16:14  lr: 0.000020  loss: 3.7579  time: 0.5646  data: 0.0002  max mem: 15371
Train Epoch: [0]  [  850/27734]  eta: 4:15:32  lr: 0.000020  loss: 3.2526  time: 0.5646  data: 0.0002  max mem: 15371
Train Epoch: [0]  [  900/27734]  eta: 4:14:59  lr: 0.000020  loss: 4.1323  time: 0.5686  data: 0.0001  max mem: 15371
Train Epoch: [0]  [  950/27734]  eta: 4:14:24  lr: 0.000020  loss: 6.6641  time: 0.5654  data: 0.0002  max mem: 15371
Train Epoch: [0]  [ 1000/27734]  eta: 4:13:34  lr: 0.000020  loss: 2.4035  time: 0.5423  data: 0.0002  max mem: 15371
Train Epoch: [0]  [ 1050/27734]  eta: 4:13:01  lr: 0.000020  loss: 5.0540  time: 0.5704  data: 0.0002  max mem: 15371
Train Epoch: [0]  [ 1100/27734]  eta: 4:12:22  lr: 0.000020  loss: 6.9347  time: 0.5604  data: 0.0002  max mem: 15371
Train Epoch: [0]  [ 1150/27734]  eta: 4:11:51  lr: 0.000020  loss: 3.6034  time: 0.5685  data: 0.0002  max mem: 15371
Train Epoch: [0]  [ 1200/27734]  eta: 4:11:17  lr: 0.000020  loss: 3.6148  time: 0.5620  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 1250/27734]  eta: 4:10:43  lr: 0.000020  loss: 4.4637  time: 0.5638  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 1300/27734]  eta: 4:10:14  lr: 0.000020  loss: 7.0503  time: 0.5650  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 1350/27734]  eta: 4:09:49  lr: 0.000020  loss: 3.8444  time: 0.5666  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 1400/27734]  eta: 4:09:15  lr: 0.000020  loss: 5.4954  time: 0.5611  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 1450/27734]  eta: 4:08:43  lr: 0.000020  loss: 2.9929  time: 0.5630  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 1500/27734]  eta: 4:08:11  lr: 0.000020  loss: 4.1236  time: 0.5671  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 1550/27734]  eta: 4:07:41  lr: 0.000020  loss: 3.7537  time: 0.5671  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 1600/27734]  eta: 4:07:13  lr: 0.000020  loss: 4.9755  time: 0.5701  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 1650/27734]  eta: 4:06:45  lr: 0.000020  loss: 3.1161  time: 0.5683  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 1700/27734]  eta: 4:06:17  lr: 0.000020  loss: 7.7229  time: 0.5729  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 1750/27734]  eta: 4:05:45  lr: 0.000020  loss: 5.1467  time: 0.5632  data: 0.0003  max mem: 15547
Train Epoch: [0]  [ 1800/27734]  eta: 4:05:19  lr: 0.000020  loss: 4.3093  time: 0.5702  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 1850/27734]  eta: 4:04:51  lr: 0.000020  loss: 4.5292  time: 0.5717  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 1900/27734]  eta: 4:04:18  lr: 0.000020  loss: 4.0122  time: 0.5601  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 1950/27734]  eta: 4:03:54  lr: 0.000020  loss: 7.1777  time: 0.5808  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 2000/27734]  eta: 4:03:23  lr: 0.000020  loss: 2.0908  time: 0.5583  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 2050/27734]  eta: 4:02:50  lr: 0.000020  loss: 4.5217  time: 0.5592  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 2100/27734]  eta: 4:02:25  lr: 0.000020  loss: 3.8436  time: 0.5795  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 2150/27734]  eta: 4:01:56  lr: 0.000020  loss: 4.3569  time: 0.5656  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 2200/27734]  eta: 4:01:28  lr: 0.000020  loss: 3.7807  time: 0.5650  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 2250/27734]  eta: 4:01:01  lr: 0.000020  loss: 3.3598  time: 0.5663  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 2300/27734]  eta: 4:00:33  lr: 0.000020  loss: 3.2008  time: 0.5745  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 2350/27734]  eta: 4:00:02  lr: 0.000020  loss: 3.0584  time: 0.5651  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 2400/27734]  eta: 3:59:34  lr: 0.000020  loss: 2.8421  time: 0.5658  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 2450/27734]  eta: 3:59:05  lr: 0.000020  loss: 3.4924  time: 0.5665  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 2500/27734]  eta: 3:58:37  lr: 0.000020  loss: 3.9569  time: 0.5703  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 2550/27734]  eta: 3:58:12  lr: 0.000020  loss: 2.5634  time: 0.5693  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 2600/27734]  eta: 3:57:44  lr: 0.000020  loss: 5.7312  time: 0.5640  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 2650/27734]  eta: 3:57:15  lr: 0.000020  loss: 2.4450  time: 0.5683  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 2700/27734]  eta: 3:56:47  lr: 0.000020  loss: 3.2882  time: 0.5749  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 2750/27734]  eta: 3:56:19  lr: 0.000020  loss: 3.9089  time: 0.5647  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 2800/27734]  eta: 3:55:49  lr: 0.000020  loss: 2.9631  time: 0.5639  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 2850/27734]  eta: 3:55:21  lr: 0.000020  loss: 2.4176  time: 0.5707  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 2900/27734]  eta: 3:54:52  lr: 0.000020  loss: 4.6060  time: 0.5704  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 2950/27734]  eta: 3:54:23  lr: 0.000020  loss: 1.8023  time: 0.5621  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 3000/27734]  eta: 3:53:53  lr: 0.000020  loss: 2.8642  time: 0.5669  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 3050/27734]  eta: 3:53:23  lr: 0.000020  loss: 2.1850  time: 0.5639  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 3100/27734]  eta: 3:52:55  lr: 0.000020  loss: 4.1691  time: 0.5715  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 3150/27734]  eta: 3:52:27  lr: 0.000020  loss: 3.4681  time: 0.5683  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 3200/27734]  eta: 3:51:59  lr: 0.000020  loss: 2.2653  time: 0.5691  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 3250/27734]  eta: 3:51:31  lr: 0.000020  loss: 3.8678  time: 0.5737  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 3300/27734]  eta: 3:51:04  lr: 0.000020  loss: 4.3111  time: 0.5695  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 3350/27734]  eta: 3:50:36  lr: 0.000020  loss: 4.5013  time: 0.5675  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 3400/27734]  eta: 3:50:07  lr: 0.000020  loss: 1.9759  time: 0.5663  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 3450/27734]  eta: 3:49:38  lr: 0.000020  loss: 4.3444  time: 0.5675  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 3500/27734]  eta: 3:49:07  lr: 0.000020  loss: 2.6801  time: 0.5574  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 3550/27734]  eta: 3:48:39  lr: 0.000020  loss: 5.6699  time: 0.5700  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 3600/27734]  eta: 3:48:10  lr: 0.000020  loss: 4.1994  time: 0.5664  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 3650/27734]  eta: 3:47:42  lr: 0.000020  loss: 4.0265  time: 0.5621  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 3700/27734]  eta: 3:47:12  lr: 0.000020  loss: 2.8360  time: 0.5669  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 3750/27734]  eta: 3:46:44  lr: 0.000020  loss: 3.3396  time: 0.5637  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 3800/27734]  eta: 3:46:15  lr: 0.000020  loss: 6.1632  time: 0.5575  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 3850/27734]  eta: 3:45:47  lr: 0.000020  loss: 3.8963  time: 0.5685  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 3900/27734]  eta: 3:45:21  lr: 0.000020  loss: 3.2404  time: 0.5742  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 3950/27734]  eta: 3:44:51  lr: 0.000020  loss: 2.3484  time: 0.5633  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 4000/27734]  eta: 3:44:21  lr: 0.000020  loss: 4.1950  time: 0.5637  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 4050/27734]  eta: 3:43:53  lr: 0.000020  loss: 2.9731  time: 0.5731  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 4100/27734]  eta: 3:43:24  lr: 0.000020  loss: 3.0791  time: 0.5792  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 4150/27734]  eta: 3:42:56  lr: 0.000020  loss: 4.7600  time: 0.5706  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 4200/27734]  eta: 3:42:27  lr: 0.000020  loss: 5.7811  time: 0.5641  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 4250/27734]  eta: 3:41:59  lr: 0.000020  loss: 4.2610  time: 0.5614  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 4300/27734]  eta: 3:41:30  lr: 0.000020  loss: 2.6399  time: 0.5566  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 4350/27734]  eta: 3:41:03  lr: 0.000020  loss: 2.3170  time: 0.5675  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 4400/27734]  eta: 3:40:33  lr: 0.000020  loss: 3.3396  time: 0.5636  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 4450/27734]  eta: 3:40:05  lr: 0.000020  loss: 5.0497  time: 0.5692  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 4500/27734]  eta: 3:39:37  lr: 0.000020  loss: 2.8623  time: 0.5656  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 4550/27734]  eta: 3:39:08  lr: 0.000020  loss: 4.1194  time: 0.5666  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 4600/27734]  eta: 3:38:39  lr: 0.000020  loss: 3.4895  time: 0.5637  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 4650/27734]  eta: 3:38:10  lr: 0.000020  loss: 3.7316  time: 0.5639  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 4700/27734]  eta: 3:37:43  lr: 0.000020  loss: 2.7990  time: 0.5662  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 4750/27734]  eta: 3:37:16  lr: 0.000020  loss: 4.4100  time: 0.5795  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 4800/27734]  eta: 3:36:46  lr: 0.000020  loss: 3.1894  time: 0.5564  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 4850/27734]  eta: 3:36:18  lr: 0.000020  loss: 2.7274  time: 0.5699  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 4900/27734]  eta: 3:35:50  lr: 0.000020  loss: 2.9761  time: 0.5724  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 4950/27734]  eta: 3:35:22  lr: 0.000020  loss: 1.7162  time: 0.5677  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 5000/27734]  eta: 3:34:55  lr: 0.000020  loss: 4.1928  time: 0.5747  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 5050/27734]  eta: 3:34:27  lr: 0.000020  loss: 3.7697  time: 0.5669  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 5100/27734]  eta: 3:33:59  lr: 0.000020  loss: 1.5427  time: 0.5758  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 5150/27734]  eta: 3:33:30  lr: 0.000020  loss: 3.7586  time: 0.5537  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 5200/27734]  eta: 3:33:01  lr: 0.000020  loss: 3.1163  time: 0.5603  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 5250/27734]  eta: 3:32:32  lr: 0.000020  loss: 2.9448  time: 0.5627  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 5300/27734]  eta: 3:32:05  lr: 0.000020  loss: 2.4167  time: 0.5678  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 5350/27734]  eta: 3:31:37  lr: 0.000020  loss: 3.4437  time: 0.5651  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 5400/27734]  eta: 3:31:08  lr: 0.000020  loss: 4.8009  time: 0.5647  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 5450/27734]  eta: 3:30:40  lr: 0.000020  loss: 2.1090  time: 0.5691  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 5500/27734]  eta: 3:30:15  lr: 0.000020  loss: 3.8758  time: 0.5807  data: 0.0003  max mem: 15547
Train Epoch: [0]  [ 5550/27734]  eta: 3:29:47  lr: 0.000020  loss: 2.6977  time: 0.5704  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 5600/27734]  eta: 3:29:22  lr: 0.000020  loss: 4.5415  time: 0.5686  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 5650/27734]  eta: 3:28:55  lr: 0.000020  loss: 3.0292  time: 0.5744  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 5700/27734]  eta: 3:28:27  lr: 0.000020  loss: 2.3229  time: 0.5488  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 5750/27734]  eta: 3:27:59  lr: 0.000020  loss: 3.8009  time: 0.5846  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 5800/27734]  eta: 3:27:32  lr: 0.000020  loss: 4.2163  time: 0.5755  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 5850/27734]  eta: 3:27:02  lr: 0.000020  loss: 1.6654  time: 0.5555  data: 0.0003  max mem: 15547
Train Epoch: [0]  [ 5900/27734]  eta: 3:26:34  lr: 0.000020  loss: 4.0156  time: 0.5595  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 5950/27734]  eta: 3:26:06  lr: 0.000020  loss: 2.9589  time: 0.5691  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 6000/27734]  eta: 3:25:39  lr: 0.000020  loss: 4.4844  time: 0.5731  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 6050/27734]  eta: 3:25:11  lr: 0.000020  loss: 2.5243  time: 0.5697  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 6100/27734]  eta: 3:24:42  lr: 0.000020  loss: 4.9789  time: 0.5632  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 6150/27734]  eta: 3:24:14  lr: 0.000020  loss: 3.3287  time: 0.5666  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 6200/27734]  eta: 3:23:44  lr: 0.000020  loss: 2.1540  time: 0.5518  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 6250/27734]  eta: 3:23:15  lr: 0.000020  loss: 3.3569  time: 0.5667  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 6300/27734]  eta: 3:22:47  lr: 0.000020  loss: 2.0722  time: 0.5745  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 6350/27734]  eta: 3:22:19  lr: 0.000020  loss: 0.7416  time: 0.5705  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 6400/27734]  eta: 3:21:50  lr: 0.000020  loss: 2.5320  time: 0.5601  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 6450/27734]  eta: 3:21:22  lr: 0.000020  loss: 2.6988  time: 0.5711  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 6500/27734]  eta: 3:20:55  lr: 0.000020  loss: 3.9473  time: 0.5769  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 6550/27734]  eta: 3:20:28  lr: 0.000020  loss: 2.8614  time: 0.5767  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 6600/27734]  eta: 3:20:01  lr: 0.000020  loss: 4.8327  time: 0.5705  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 6650/27734]  eta: 3:19:32  lr: 0.000020  loss: 4.5459  time: 0.5765  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 6700/27734]  eta: 3:19:05  lr: 0.000020  loss: 4.1884  time: 0.5748  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 6750/27734]  eta: 3:18:37  lr: 0.000020  loss: 2.9231  time: 0.5690  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 6800/27734]  eta: 3:18:09  lr: 0.000020  loss: 1.6913  time: 0.5738  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 6850/27734]  eta: 3:17:42  lr: 0.000020  loss: 3.4388  time: 0.5758  data: 0.0001  max mem: 15547
Train Epoch: [0]  [ 6900/27734]  eta: 3:17:13  lr: 0.000020  loss: 2.5510  time: 0.5706  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 6950/27734]  eta: 3:16:46  lr: 0.000020  loss: 2.7216  time: 0.5749  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 7000/27734]  eta: 3:16:19  lr: 0.000020  loss: 2.3912  time: 0.5949  data: 0.0002  max mem: 15547
Train Epoch: [0]  [ 7050/27734]  eta: 3:15:52  lr: 0.000020  loss: 2.3877  time: 0.5675  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 7100/27734]  eta: 3:15:24  lr: 0.000020  loss: 2.2142  time: 0.5621  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 7150/27734]  eta: 3:14:56  lr: 0.000020  loss: 1.7465  time: 0.5793  data: 0.0001  max mem: 15689
Train Epoch: [0]  [ 7200/27734]  eta: 3:14:28  lr: 0.000020  loss: 5.1862  time: 0.5711  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 7250/27734]  eta: 3:14:00  lr: 0.000020  loss: 3.3989  time: 0.5705  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 7300/27734]  eta: 3:13:32  lr: 0.000020  loss: 2.6640  time: 0.5697  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 7350/27734]  eta: 3:13:05  lr: 0.000020  loss: 2.6714  time: 0.5754  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 7400/27734]  eta: 3:12:37  lr: 0.000020  loss: 3.6537  time: 0.5762  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 7450/27734]  eta: 3:12:09  lr: 0.000020  loss: 4.5415  time: 0.5701  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 7500/27734]  eta: 3:11:40  lr: 0.000020  loss: 3.2815  time: 0.5630  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 7550/27734]  eta: 3:11:12  lr: 0.000020  loss: 4.0682  time: 0.5668  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 7600/27734]  eta: 3:10:43  lr: 0.000020  loss: 2.4448  time: 0.5718  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 7650/27734]  eta: 3:10:15  lr: 0.000020  loss: 3.4022  time: 0.5736  data: 0.0001  max mem: 15689
Train Epoch: [0]  [ 7700/27734]  eta: 3:09:46  lr: 0.000020  loss: 3.4772  time: 0.5656  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 7750/27734]  eta: 3:09:18  lr: 0.000020  loss: 2.9582  time: 0.5719  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 7800/27734]  eta: 3:08:50  lr: 0.000020  loss: 3.4015  time: 0.5705  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 7850/27734]  eta: 3:08:22  lr: 0.000020  loss: 2.4735  time: 0.5644  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 7900/27734]  eta: 3:07:53  lr: 0.000020  loss: 2.1093  time: 0.5701  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 7950/27734]  eta: 3:07:24  lr: 0.000020  loss: 3.5321  time: 0.5668  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 8000/27734]  eta: 3:06:56  lr: 0.000020  loss: 2.8158  time: 0.5734  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 8050/27734]  eta: 3:06:28  lr: 0.000020  loss: 3.9952  time: 0.5732  data: 0.0001  max mem: 15689
Train Epoch: [0]  [ 8100/27734]  eta: 3:06:00  lr: 0.000020  loss: 2.2064  time: 0.5717  data: 0.0001  max mem: 15689
Train Epoch: [0]  [ 8150/27734]  eta: 3:05:31  lr: 0.000020  loss: 2.7743  time: 0.5746  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 8200/27734]  eta: 3:05:02  lr: 0.000020  loss: 2.3864  time: 0.5691  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 8250/27734]  eta: 3:04:34  lr: 0.000020  loss: 3.8928  time: 0.5688  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 8300/27734]  eta: 3:04:06  lr: 0.000020  loss: 2.2320  time: 0.5687  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 8350/27734]  eta: 3:03:38  lr: 0.000020  loss: 3.9246  time: 0.5629  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 8400/27734]  eta: 3:03:10  lr: 0.000020  loss: 1.4765  time: 0.5716  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 8450/27734]  eta: 3:02:42  lr: 0.000020  loss: 2.8969  time: 0.5664  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 8500/27734]  eta: 3:02:13  lr: 0.000020  loss: 4.4395  time: 0.5656  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 8550/27734]  eta: 3:01:45  lr: 0.000020  loss: 4.2771  time: 0.5665  data: 0.0001  max mem: 15689
Train Epoch: [0]  [ 8600/27734]  eta: 3:01:16  lr: 0.000020  loss: 4.0481  time: 0.5683  data: 0.0001  max mem: 15689
Train Epoch: [0]  [ 8650/27734]  eta: 3:00:48  lr: 0.000020  loss: 3.8173  time: 0.5689  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 8700/27734]  eta: 3:00:20  lr: 0.000020  loss: 2.4501  time: 0.5650  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 8750/27734]  eta: 2:59:51  lr: 0.000020  loss: 3.9041  time: 0.5629  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 8800/27734]  eta: 2:59:22  lr: 0.000020  loss: 3.1465  time: 0.5641  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 8850/27734]  eta: 2:58:54  lr: 0.000020  loss: 3.4956  time: 0.5693  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 8900/27734]  eta: 2:58:25  lr: 0.000020  loss: 3.4331  time: 0.5701  data: 0.0001  max mem: 15689
Train Epoch: [0]  [ 8950/27734]  eta: 2:57:57  lr: 0.000020  loss: 3.5290  time: 0.5625  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 9000/27734]  eta: 2:57:29  lr: 0.000020  loss: 3.3199  time: 0.5700  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 9050/27734]  eta: 2:57:01  lr: 0.000020  loss: 3.3837  time: 0.5723  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 9100/27734]  eta: 2:56:33  lr: 0.000020  loss: 2.9071  time: 0.5701  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 9150/27734]  eta: 2:56:04  lr: 0.000020  loss: 3.1977  time: 0.5747  data: 0.0001  max mem: 15689
Train Epoch: [0]  [ 9200/27734]  eta: 2:55:36  lr: 0.000020  loss: 2.0719  time: 0.5661  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 9250/27734]  eta: 2:55:08  lr: 0.000020  loss: 2.4147  time: 0.5686  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 9300/27734]  eta: 2:54:40  lr: 0.000020  loss: 2.7888  time: 0.5744  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 9350/27734]  eta: 2:54:12  lr: 0.000020  loss: 3.1144  time: 0.5648  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 9400/27734]  eta: 2:53:44  lr: 0.000020  loss: 1.4000  time: 0.5716  data: 0.0001  max mem: 15689
Train Epoch: [0]  [ 9450/27734]  eta: 2:53:16  lr: 0.000020  loss: 2.7764  time: 0.5692  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 9500/27734]  eta: 2:52:48  lr: 0.000020  loss: 2.8164  time: 0.5730  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 9550/27734]  eta: 2:52:20  lr: 0.000020  loss: 4.8405  time: 0.5809  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 9600/27734]  eta: 2:51:51  lr: 0.000020  loss: 6.2593  time: 0.5680  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 9650/27734]  eta: 2:51:23  lr: 0.000020  loss: 3.6791  time: 0.5678  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 9700/27734]  eta: 2:50:54  lr: 0.000020  loss: 2.5444  time: 0.5629  data: 0.0002  max mem: 15689
Train Epoch: [0]  [ 9750/27734]  eta: 2:50:25  lr: 0.000020  loss: 2.4643  time: 0.5640  data: 0.0002  max mem: 15689